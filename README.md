# oneChatGPT
- oneChatbot_gpt2-vietnamese_fine-tune.py
- Author: Mr.Jack _ C√¥ng ty www.BICweb.vn
- Date: 24 August 2023

chatGPT si√™u si√™u nh·ªè, hu·∫•n luy·ªán ch·ªâ v·ªõi 1 c√¢u duy nh·∫•t, v√† ch·ªâ trong 1 ph√∫t (chatGPT super super tiny ... training with only one sentence dataset in one minute !). N·∫øu b·∫°n th·∫•y th√∫ v·ªã th√¨ **h√£y th·∫£ sao** ƒë·ªÉ ·ªßng h·ªô m√¨nh nh√© ^^

**But what is a GPT? Visual intro to Transformers** | Deep learning, chapter 5 (https://www.youtube.com/watch?v=wjZofJX0v4M)

"V√†o ng√†y 21/08/2023, C√¥ng ty C·ªï ph·∫ßn VinBigdata c√¥ng b·ªë x√¢y d·ª±ng th√†nh c√¥ng m√¥ h√¨nh ng√¥n ng·ªØ l·ªõn ti·∫øng Vi·ªát, ƒë·∫∑t n·ªÅn m√≥ng cho vi·ªác x√¢y d·ª±ng c√°c gi·∫£i ph√°p t√≠ch h·ª£p AI t·∫°o sinh..." 
https://genk.vn/vinbigdata-phat-trien-cong-nghe-ai-tao-sinh-se-som-cho-ra-mat-chatgpt-phien-ban-viet-20230821140700664.chn

ƒê√¢y c≈©ng c√≥ th·ªÉ coi l√† m·ªôt s·ª± ki·ªán l·ªõn c·ªßa d√¢n ng√†nh IT n√≥i chung v√† d√¢n l·∫≠p tr√¨nh "ng√†nh" chatGPT n√≥i ri√™ng, v√† ch·∫Øc c√≥ l·∫Ω "d√¢n ng√†nh" n√†o c≈©ng mong mu·ªën l√†m ƒë∆∞·ª£c ra m·ªôt em chatGPT nh∆∞ v·∫≠y ^^

Nh∆∞ng ƒë·ªëi v·ªõi k·ªπ s∆∞ l·∫≠p tr√¨nh khi m·ªõi b∆∞·ªõc v√†o th·∫ø gi·ªõi chatGPT th√¥ng th∆∞·ªùng s·∫Ω c√≥ nh·ªØng bƒÉn khoƒÉn sau:
1. M√¨nh c√≥ th·ªÉ tri·ªÉn khai ƒë∆∞·ª£c 1 em chatGPT ch·∫°y ƒë∆∞·ª£c ·ªü nh√† kh√¥ng?
2. C·∫ßn chu·∫©n b·ªã m√°y m√≥c c·∫•u h√¨nh ph·∫£i m·∫°nh c·ª° n√†o?
3. Kh·ªëi d·ªØ li·ªáu chu·∫©n b·ªã hu·∫•n luy·ªán l·ªõn ƒë·∫øn ƒë√¢u?
4. Ti·∫øng Vi·ªát th√¨ c√≥ kh√≥ hu·∫•n luy·ªán kh√¥ng?
5. Ph∆∞∆°ng ph√°p hu·∫•n luy·ªán ra sao?
6. Th·ªùi gian hu·∫•n luy·ªán trong bao l√¢u? 
7. ƒê·ªô ch√≠nh x√°c c√¢u tr·∫£ l·ªùi ra sao?
8. Bao gi·ªù s·∫Ω th·ª±c hi·ªán ƒë∆∞·ª£c ƒëi·ªÅu ƒë√≥?

==> Th√¥ng th∆∞·ªùng th√¨ c√¢u tr·∫£ l·ªùi s·∫Ω l√† ... Kh√¥ng bi·∫øt ^^


V√¨ v·∫≠y m√† h√¥m nay m√¨nh chia s·∫ª v·ªõi c√°c b·∫°n m·ªôt em chatGPT si√™u.. si√™u.. si√™u nh·ªè, ƒë·ªÉ c√°c b·∫°n c√πng t√¨m hi·ªÉu v√† ch∆°i v·ªõi em n√≥ nh√© ^^

- M·ª•c ƒë√≠ch: Nghi√™n c·ª©u h·ªçc t·∫≠p
- Ng√¥n ng·ªØ l·∫≠p tr√¨nh: Python
- ƒê·ªô d√†i m√£ ngu·ªìn: 55 d√≤ng code ^^
- Model pretrained: GPT2

- Ng√¥n ng·ªØ hu·∫•n luy·ªán: Ti·∫øng Vi·ªát
- D·ªØ li·ªáu hu·∫•n luy·ªán: Ch·ªâ 01 c√¢u duy nh·∫•t ^^
- M√°y hu·∫•n luy·ªán: 01 laptop sinh vi√™n
- Th·ªùi gian hu·∫•n luy·ªán: 01 ph√∫t
- ƒê·ªô tr·∫£ l·ªùi ch√≠nh x√°c: 100%

ƒê·ªÉ th·ª≠ nghi·ªám h√£y download file v√† ch·∫°y c√¢u l·ªánh:
$ python oneChatbot_gpt2-vietnamese_fine-tune.py

![alt text](https://github.com/Mr-Jack-Tung/oneChatGPT/blob/main/oneChatbot_Screenshot%202023-08-24%20at%2011.30.png)

Hy v·ªçng ƒëi·ªÅu n√†y s·∫Ω gi√∫p c√°c b·∫°n th√™m t·ª± tin tr√™n con ƒë∆∞·ªùng l·∫≠p tr√¨nh chinh ph·ª•c chatGPT nh√©! ^^<br>

------------------------------
**Update**: Monday,25/09/2023 ~> Model c√≥ bao nhi√™u Parameters l√† ƒë·ªß ƒë·ªÉ fine-tune ch·ªâ 01 c√¢u ti·∫øng Vi·ªát ch√≠nh x√°c ?

M√¨nh ƒë√£ th·ª≠ fine-tune v·ªõi model 'huggingface.co/roneneldan/TinyStories-33M', ch·ªâ v·ªõi 4 x GPTNeoBlock(features=768), c√≥ s·ªë l∆∞·ª£ng tham s·ªë l√† 33 tri·ªáu, nh·ªè h∆°n nhi·ªÅu so v·ªõi model gpt2 (124M), ƒë∆∞·ª£c ƒë√°nh gi√° l√† c√≥ ch·∫•t l∆∞·ª£ng kh√° t·ªët v·ªõi t·ªëc ƒë·ªô train nhanh h∆°n (https://arxiv.org/abs/2305.07759), nh∆∞ng khi th·ª≠ nghi·ªám th√¨ k·∫øt qu·∫£ c≈©ng ·ªïn v·ªõi model 'TinyStories-33M', c√≤n c√°c model v·ªõi s·ªë l∆∞·ª£ng params √≠t h∆°n (nh∆∞ model 1M, 3M, 8M, 21M) th√¨ kh√¥ng ok ^^

Update code:
- from transformers import AutoTokenizer, AutoModelForCausalLM
- model = AutoModelForCausalLM.from_pretrained('roneneldan/TinyStories-33M')
- tokenizer = AutoTokenizer.from_pretrained('EleutherAI/gpt-neo-125M')

Result:
- model_name = 'roneneldan/TinyStories-33M'
- lr=5e-4
- (0.665s) Epoch 29, Loss 0.115172
- Question: Xin ch√†o Answer: C√¥ng ty BICweb k√≠nh ch√†o qu√Ω kh√°ch!

With Emoji:
- model_name = 'roneneldan/TinyStories-33M'
- lr=6e-4
- qa_pair = 'Question: Xin ch√†o Answer: C√¥ng ty BICweb k√≠nh ch√†o qu√Ω kh√°ch ü§ó.'
- (0.662s) Epoch 49, Loss 0.099960
- Question: Xin ch√†o Answer: C√¥ng ty BICweb k√≠nh ch√†o qu√Ω kh√°ch ü§ó

Extra 4fun:
- model_name = 'roneneldan/TinyStories-1M'
- The best learning rate/ loss nh∆∞ng k·∫øt qu·∫£ v·∫´n r·∫•t t·ªá :(
- lr=4.3294e-3
- Epoch 143, Loss 1.087497
- Question: Xin ch√†o Answer: CICweb k khbeh: C ty BICh ch√†o B√°ch!

------------------------------
**Update**: Sunday,15/10/2023 ~> C√≥ th·ªÉ hu·∫•n luy·ªán cho model GPT2 hi·ªÉu ƒë∆∞·ª£c h√¨nh ·∫£nh kh√¥ng?
(chatGPT super super tiny ... training with only one image dataset in one minute !)

Ng√†y 25/09/2023 v·ª´a r·ªìi OpenAI c√≥ th√¥ng b√°o l√† con ChatBot c·ªßa h·ªç c√≥ th·ªÉ nh√¨n, nghe, v√† n√≥i ƒë∆∞·ª£c (https://openai.com/blog/chatgpt-can-now-see-hear-and-speak), ƒëi·ªÅu n√†y c≈©ng th√∫c ƒë·∫©y m√¨nh th·ª≠ nghi·ªám xem model GPT2 c√≥ th·ªÉ nh·∫≠n di·ªán ƒë∆∞·ª£c h√¨nh ·∫£nh kh√¥ng. V√† m√¨nh ƒë√£ th·ª≠ fine-turn model 'huggingface.co/nlpconnect/vit-gpt2-image-captioning' ƒë·ªÉ nh·∫≠n di·ªán ƒë∆∞·ª£c h√¨nh ·∫£nh v√† tr·∫£ l·ªùi b·∫±ng Ti·∫øng Vi·ªát. K·∫øt qu·∫£ kh√° t·ªët nh∆∞ sau:

Result:
- model_name = 'nlpconnect/vit-gpt2-image-captioning'
- lr=5e-4
- (3.083s) Epoch 15, Loss 0.033
- Answer: ƒê√¢y l√† c·ªù Vi·ªát Nam!

ƒê·ªÉ th·ª≠ nghi·ªám h√£y download file v√† ch·∫°y c√¢u l·ªánh: $ python oneChatbot_vit-gpt2-image-captioning-vietnamese_fine-tune.py

L∆∞u √Ω: pip install transformers==4.25.1

![alt text](https://github.com/Mr-Jack-Tung/oneChatGPT/blob/main/oneChatbot-vit_Screenshot%202023-10-15%20at%208.26%20PM.png)

------------------------------
**Update**: Sunday,29/10/2023 ~> T·∫°o giao di·ªán Chat ch·∫°y Local cho model GPT2 th·∫≠t ƒë∆°n gi·∫£n v·ªõi Gradio ^^

ƒê·ªÉ th·ª≠ nghi·ªám h√£y download file v√† ch·∫°y c√¢u l·ªánh:

$ python gpt2-gradio.py

![alt text](https://github.com/Mr-Jack-Tung/oneChatGPT/blob/main/GPT2-Gradio_Screenshot%202023-10-29%20at%205.08.png)

------------------------------
**Update**: Monday,30/10/2023 ~> Quay tr·ªü l·∫°i c√¢u h·ªèi: Model c√≥ bao nhi√™u Parameters l√† ƒë·ªß ƒë·ªÉ fine-tune ch·ªâ 01 c√¢u ti·∫øng Vi·ªát ch√≠nh x√°c ?

H√¥m tr∆∞·ªõc khi m√¨nh ƒë·ªçc c√°i paper: Pretraining on the Test Set Is All You Need (https://arxiv.org/abs/2309.08632) th√¨ t·ª± d∆∞ng th·∫•y kh√° l√† ·∫•m ·ª©c khi kh√¥ng train ƒë∆∞·ª£c c√°i model n√†o nh·ªè c·ª° 1M parameters m√† v·∫´n tr·∫£ l·ªùi ƒë∆∞·ª£c ch√≠nh x√°c. Trong khi 'roneneldan/TinyStories-1M' (https://arxiv.org/abs/2305.07759) l√†m ƒë∆∞·ª£c v√† g·∫ßn ƒë√¢y nh·∫•t l√† phi-CTNL (https://arxiv.org/abs/2309.08632) h·ªç l√†m ƒë∆∞·ª£c, m√† m√¨nh ch·ªâ train ƒë∆∞·ª£c ch√≠nh x√°c v·ªõi model nh·ªè nh·∫•t l√† TinyStories-33M :(

H√¥m nay m√¨nh quay tr·ªü l·∫°i quy·∫øt t√¢m chinh ph·ª•c vi·ªác Fine-tune b·∫±ng ƒë∆∞·ª£c c√°i model 1M ti·∫øng Vi·ªát v·ªõi dataset l√† 1 c√¢u duy nh·∫•t. Theo kinh nghi·ªám, m√¨nh ti·∫øp t·ª•c s·ª≠ d·ª•ng model 'roneneldan/TinyStories-1M' ƒë·ªÉ fine-tune. Sau r·∫•t nhi·ªÅu l·∫ßn th·∫•t b·∫°i, m·∫•t nguy√™n c·∫£ m·ªôt c√°i bu·ªïi chi·ªÅu ch·ªß nh·∫≠t ƒë·∫πp tr·ªùi, cu·ªëi c√πng th√¨ "Tr·ªùi c≈©ng kh√¥ng ph·ª• l√≤ng ng∆∞·ªùi" (^.^) m√¨nh ƒë√£ t√¨m ra ƒë∆∞·ª£c c√¥ng th·ª©c ƒë·ªÉ fine-tune model si√™u si√™u nh·ªè 1M parameters (8 x GPTNeoBlock, features=64). K·∫øt qu·∫£ r·∫•t t·ªët nh∆∞ sau:

Update code:
- tokenizer = AutoTokenizer.from_pretrained("EleutherAI/gpt-neo-125M")
- model = AutoModelForCausalLM.from_pretrained('roneneldan/TinyStories-1M')

- optimizer = torch.optim.AdamW(model.parameters(), lr=0.05)
- scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.99)

Result:
- model_name = 'roneneldan/TinyStories-1M'
- lr=0.05
- gamma=0.99
- (0.022s) Epoch 143, Loss 0.009
- Question: Xin ch√†o Answer: C√¥ng ty BICweb k√≠nh ch√†o qu√Ω kh√°ch!

ƒê·ªÉ th·ª≠ nghi·ªám h√£y download file v√† ch·∫°y c√¢u l·ªánh:

$ python oneChatbot_TinyGPT-1M_vietnamese_fine-tune.py

![alt text](https://github.com/Mr-Jack-Tung/oneChatGPT/blob/main/oneChatbot_TinyGPT-1M_vietnamese_fine-tune%20_%20Screenshot%202023-10-30%20at%208.05PM.png)

Model: roneneldan/TinyStories-1M

GPTNeoForCausalLM(
  (transformer): GPTNeoModel(
    (wte): Embedding(50257, 64)
    (wpe): Embedding(2048, 64)
    (drop): Dropout(p=0.0, inplace=False)
    (h): ModuleList(
      (0-7): 8 x GPTNeoBlock(
        (ln_1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (attn): GPTNeoAttention(
          (attention): GPTNeoSelfAttention(
            (attn_dropout): Dropout(p=0.0, inplace=False)
            (resid_dropout): Dropout(p=0.0, inplace=False)
            (k_proj): Linear(in_features=64, out_features=64, bias=False)
            (v_proj): Linear(in_features=64, out_features=64, bias=False)
            (q_proj): Linear(in_features=64, out_features=64, bias=False)
            (out_proj): Linear(in_features=64, out_features=64, bias=True)
          )
        )
        (ln_2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (mlp): GPTNeoMLP(
          (c_fc): Linear(in_features=64, out_features=256, bias=True)
          (c_proj): Linear(in_features=256, out_features=64, bias=True)
          (act): NewGELUActivation()
          (dropout): Dropout(p=0.0, inplace=False)
        )
      )
    )
    (ln_f): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
  )
  (lm_head): Linear(in_features=64, out_features=50257, bias=False)
)

------------------------------

**Update** Tuesday, 02 April 2024: Phi√™n b·∫£n ƒë∆∞·ª£c ch·ªânh s·ª≠a b·ªüi ChatGPT 3.5 ^^
- Rewrite version: oneChatbot_gpt2-vietnamese_fine-tune_(rewrite_by_ChatGPT).py
- Simplify version: oneChatbot_gpt2-vietnamese_fine-tune_(rewrite_by_ChatGPT)_v2.py

------------------------------
**Update** saving model: 25 May 2024<br>
File: oneChatbot_gpt2-vietnamese_fine-tune.py<br>
Code:<br>
print("\nSaving the model...")<br>
OUTPUT_MODEL = 'OneChatbotGPT2Vi'<br>
tokenizer.save_pretrained(OUTPUT_MODEL)<br>
model.save_pretrained(OUTPUT_MODEL)<br>

**Update** using SFTTrainer: 25 May 2024<br>
S·ª≠ d·ª•ng SFTTrainer c·ªßa Hugging Face ƒë·ªÉ Fine-tune model (15 epochs), k·∫øt qu·∫£ tr·∫£ ra t·ªët<br>
File: Finetune_SFTTrainer_OneChatbotGPT2Vi.py<br>
Screenshot: oneChatbot_Finetune_SFTTrainer_Screenshot 2024-05-25.jpg<br>
![alt text](https://github.com/Mr-Jack-Tung/oneChatGPT/blob/main/oneChatbot_Finetune_SFTTrainer_Screenshot%202024-05-25.jpg)

------------------------------
**Update** using SFTTrainer with LoRA to Fine-tune: 26 May 2024<br>
S·ª≠ d·ª•ng SFTTrainer v·ªõi LoRA c·ªßa Hugging Face ƒë·ªÉ Fine-tune model, k·∫øt qu·∫£ tr·∫£ ra t·ªët v·ªõi rank=128 ^^. V√¨ model d√πng GPT2 kh√° nh·ªè n√™n khi Fine-tune v·ªõi LoRA th√¨ ph·∫£i train tƒÉng s·ªë l·∫ßn (50 epochs) v√† tƒÉng rank cao (r=128), v·ªõi c√°c m·ª©c ƒë·ªô nh·ªè h∆°n k·∫øt qu·∫£ tr·∫£ ra s·∫Ω kh√¥ng ƒë√∫ng<br><br>

M√¨nh ƒë√£ th·ª≠ SFTTrainer v·ªõi LoRA nh∆∞ng v√¨ model kh√° nh·ªè (GPT2-137M params) n√™n ƒë·ªÉ c√≥ k·∫øt qu·∫£ t·ªët th√¨ ph·∫£i ch·∫°y l·∫°i nhi·ªÅu epochs ~ 50-150 ; v√† v·ªõi rank cao ~ 64-128 tr·ªü l√™n. Th·ª≠ nghi·ªám l·∫°i th√™m v·ªõi rank=64 v√† epochs=150 ~> Ok ^^ <br>
(Ok) RANK: r=64 ; epochs=150 ; checkpoint file: ~117MB ; adapter_model.safetensors: ~37.8MB; with target_modules: ["attn.c_attn", "attn.c_proj", "mlp.c_fc", "mlp.c_proj", ]<br>
trainable params: 9,437,184 || all params: 133,876,992 || trainable%: 7.049145532041831<br><br>

Note: ƒë·ªÉ test SFTTrainer v·ªõi LoRA th√¨ v·∫´n kh√¥ng c·∫ßn ƒë·∫øn GPU, ch·ªâ c·∫ßn Laptop sinh vi√™n th√¨ v·∫´n c√≥ th·ªÉ test ƒë∆∞·ª£c nh√© ^^ <br><br>

File: Finetune_SFTTrainer_withLoRA_OneChatbotGPT2Vi.py<br>
Screenshot: oneChatbot_Finetune_SFTTrainer_withLoRA_Screenshot 2024-05-26.jpg<br>
![alt text](https://github.com/Mr-Jack-Tung/oneChatGPT/blob/main/oneChatbot_Finetune_SFTTrainer_withLoRA_Screenshot%202024-05-26.jpg)

**Update** FT with LoRA rank nh·ªè (r=16 ; lora_alpha=32)
V·ªõi ni·ªÅm tin v√† kinh nghi·ªám ƒë√£ train chatbot model si√™u nh·ªè ch·ªâ v·ªõi 1M params, n√™n m√¨nh v·∫´n th·ª≠ FT v·ªõi rank nh·ªè xem sao.<br>
(Ok) RANK: r=16 ; lora_alpha=32 ; epochs=100 ; checkpoint file: ~32MB ; adapter_model.safetensors: ~9.4MB; with target_modules: ["attn.c_attn", "attn.c_proj", "mlp.c_fc", "mlp.c_proj", ]<br>
trainable params: 2,359,296 || all params: 126,799,104 || trainable%: 1.8606566809809635<br><br>

~> hehe, oh hay th·∫≠t, v·ªõi ni·ªÅm tin v√† kinh nghi·ªám train chatbot model si√™u nh·ªè ch·ªâ v·ªõi 1M params ƒë√£ ƒë√∫ng :d fine-tune GPT2-137M model v·ªõi SFTTrainer v√† LoRA (r=16 ; lora_alpha=32, adapter_model.safetensors: ~9.4MB, trainable params: 2.36M); dataset ch·ªâ 1 c√¢u duy nh·∫•t; kh√¥ng c√≥ GPU th√¨ v·∫´n Ok nh√© üòÇ<br>

![alt text](https://github.com/Mr-Jack-Tung/oneChatGPT/blob/main/oneChatbot_Finetune_SFTTrainer_withLoRA_r16_Screenshot%202024-05-26.jpg)
